{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0889ddf",
   "metadata": {},
   "source": [
    "# Face Verification using Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554af977",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5767772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4cc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tensorflow Modules\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer,Conv2D,MaxPooling2D,Input,Flatten,Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe57209",
   "metadata": {},
   "source": [
    "## Folder Structure & Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17999ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH=os.path.join('data','positive')\n",
    "NEG_PATH=os.path.join('data','negative')\n",
    "ANC_PATH=os.path.join('data','anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44df7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAke dIrectories\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f346cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c173fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving the lfw dataset into the negative \n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw',directory)):\n",
    "        EX_PATH=os.path.join('lfw',directory,file)\n",
    "        NEW_PATH=os.path.join(NEG_PATH,file)\n",
    "        os.replace(EX_PATH,NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0308ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57d6b4",
   "metadata": {},
   "source": [
    "### Collecting images data Positive & Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c35c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    #Getting 250x250 pixel frame\n",
    "    frame=frame[120:120+250,200:200+250]\n",
    "    \n",
    "    \n",
    "    #Collecting anchor images\n",
    "    if cv2.waitKey(1) & 0XFF==ord('a'):\n",
    "        imgname=os.path.join(ANC_PATH,'{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname,frame)\n",
    "    \n",
    "    \n",
    "    #Collecting positive images\n",
    "    if cv2.waitKey(1) & 0XFF==ord('p'):\n",
    "        imgname=os.path.join(POS_PATH,'{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname,frame)\n",
    "    \n",
    "    cv2.imshow('Image Collection',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96fcfe",
   "metadata": {},
   "source": [
    "# 3 Load & Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc54075",
   "metadata": {},
   "source": [
    "### 3.1 Get directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66564889",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor=tf.data.Dataset.list_files(ANC_PATH+'\\*.jpg').take(300)\n",
    "positive=tf.data.Dataset.list_files(POS_PATH+'\\*.jpg').take(300)\n",
    "negative=tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cb9cb",
   "metadata": {},
   "source": [
    "### 3.2 Preprocess- Scale & Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b544d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img=tf.io.read_file(file_path)\n",
    "    img=tf.io.decode_jpeg(byte_img)\n",
    "    img=tf.image.resize(img,(100,100))\n",
    "    img=img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629935d6",
   "metadata": {},
   "source": [
    "### 3.3 Creating labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3476c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives=tf.data.Dataset.zip((anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives=tf.data.Dataset.zip((anchor,negative,tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data=positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746ecf7",
   "metadata": {},
   "source": [
    "### 3.4 Build Train & Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef82ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_image,validation_image,label):\n",
    "    return (preprocess(input_image), preprocess(validation_image),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70b9194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building data pipeline\n",
    "data=data.map(preprocess_twin)\n",
    "data=data.cache()\n",
    "data=data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "823b1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Partition\n",
    "train_data=data.take(round(len(data)*.7))\n",
    "train_data=train_data.batch(16)\n",
    "train_data=train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2026798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Partition\n",
    "test_data=data.skip(round(len(data)*.7))\n",
    "test_data=test_data.take(round(len(data)*.3))\n",
    "test_data=test_data.batch(16)\n",
    "test_data=test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff3487",
   "metadata": {},
   "source": [
    "## 4 Model Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963d8bc",
   "metadata": {},
   "source": [
    "### 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44c2def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "def make_embedding():\n",
    "    inp=Input(shape=(100,100,3),name='input_image')\n",
    "    \n",
    "    #First Block\n",
    "    c1=Conv2D(64,(10,10),activation='relu')(inp)\n",
    "    m1=MaxPooling2D(64,(2,2),padding='same')(c1)\n",
    "    \n",
    "    #Second block\n",
    "    c2=Conv2D(128,(7,7),activation='relu')(m1)\n",
    "    m2=MaxPooling2D(64,(2,2),padding='same')(c2)\n",
    "    \n",
    "    #third block\n",
    "    c3=Conv2D(128,(4,4),activation='relu')(m2)\n",
    "    m3=MaxPooling2D(64,(2,2),padding='same')(c3)\n",
    "    \n",
    "    #Final Embedding block\n",
    "    c4=Conv2D(256,(4,4),activation='relu')(m3)\n",
    "    f1=Flatten()(c4)\n",
    "    d1=Dense(4096,activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp],outputs=[d1],name='embedding')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "810384be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 91, 91, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding=make_embedding()\n",
    "#Use embedding.summary() to checkout whole model\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e027a90",
   "metadata": {},
   "source": [
    "### 4.2 L1 Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "045a9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class to calculate L1 distance using tensorflow\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self,input_embedding,validation_embedding):\n",
    "        return tf.math.abs(input_embedding-validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a743",
   "metadata": {},
   "source": [
    "### 4.3 Creating Siamese Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee6791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    #Anchor inputs\n",
    "    input_image=Input(name='input_image',shape=(100,100,3))\n",
    "    \n",
    "    #Validation image Inputs\n",
    "    validation_image=Input(name='validation_image',shape=(100,100,3))\n",
    "    \n",
    "    siamese_layer=L1Dist()\n",
    "    siamese_layer._name='distance'\n",
    "    distance=siamese_layer(embedding(input_image),embedding(validation_image))\n",
    "    \n",
    "    #classification layer\n",
    "    classifier=Dense(1,activation='sigmoid')(distance)\n",
    "    \n",
    "    return Model(inputs=[input_image,validation_image],outputs=classifier,name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de933231",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model=make_siamese_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f76d4c",
   "metadata": {},
   "source": [
    "## 5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e5a038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss=tf.losses.BinaryCrossentropy()\n",
    "opt=tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4404b09",
   "metadata": {},
   "source": [
    "### 5.2 Establish checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2143c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir='/training_checkpoints'\n",
    "checkpoint_prefix=os.path.join(checkpoint_dir,'ckpt')\n",
    "checkpoint=tf.train.Checkpoint(opt=opt,siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f92bff",
   "metadata": {},
   "source": [
    "### 5.3 Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2dfcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        X=batch[:2] # Get anchor Postive/Negtive images\n",
    "\n",
    "        y=batch[2] # Get the labels\n",
    "\n",
    "        yhat=siamese_model(X, training=True)\n",
    "\n",
    "        loss=binary_cross_loss(y, yhat)\n",
    "\n",
    "        #Calculate Gradient\n",
    "        grad=tape.gradient(loss,siamese_model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad,siamese_model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        #Returning the losses\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83fe9c",
   "metadata": {},
   "source": [
    "### 5.4 Build Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2eb4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,EPOCHS):\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        print('\\n Epochs {}/{}'.format(epoch,EPOCHS))\n",
    "        progbar=tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        #Looping through each batch\n",
    "        for idx,batch in enumerate(data):\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "            \n",
    "        #Saving the checkpoints\n",
    "        if epoch%10==0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "689fe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd5e5345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs 1/50\n",
      "27/27 [==============================] - 586s 22s/step\n",
      "\n",
      " Epochs 2/50\n",
      "27/27 [==============================] - 593s 22s/step\n",
      "\n",
      " Epochs 3/50\n",
      "27/27 [==============================] - 740s 28s/step\n",
      "\n",
      " Epochs 4/50\n",
      "27/27 [==============================] - 602s 22s/step\n",
      "\n",
      " Epochs 5/50\n",
      "27/27 [==============================] - 887s 33s/step\n",
      "\n",
      " Epochs 6/50\n",
      "27/27 [==============================] - 577s 21s/step\n",
      "\n",
      " Epochs 7/50\n",
      "27/27 [==============================] - 577s 21s/step\n",
      "\n",
      " Epochs 8/50\n",
      "27/27 [==============================] - 582s 22s/step\n",
      "\n",
      " Epochs 9/50\n",
      "27/27 [==============================] - 604s 22s/step\n",
      "\n",
      " Epochs 10/50\n",
      "27/27 [==============================] - 659s 24s/step\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10132/1551767335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10132/1554938483.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, EPOCHS)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#Saving the checkpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m   2168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m       \u001b[0mcheckpoint_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2170\u001b[1;33m     file_path = self.write(\"%s-%d\" % (file_prefix, checkpoint_number),\n\u001b[0m\u001b[0;32m   2171\u001b[0m                            options=options)\n\u001b[0;32m   2172\u001b[0m     checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m   2069\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2070\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcheckpoint_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckpointOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2071\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2072\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2073\u001b[0m     _checkpoint_write_durations.get_cell(\"V2\").add(\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[0m\u001b[0;32m   1263\u001b[0m         file_prefix_tensor, object_graph_tensor, options)\n\u001b[0;32m   1264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1207\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mtf_function_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msave_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m           \u001b[1;31m# merged, attempts to delete the temporary directory,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m           \u001b[1;31m# \"<user-fed prefix>_temp\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m           return gen_io_ops.merge_v2_checkpoints(\n\u001b[0m\u001b[0;32m    287\u001b[0m               sharded_prefixes, file_prefix, delete_old_dirs=True)\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name)\u001b[0m\n\u001b[0;32m    499\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m       return merge_v2_checkpoints_eager_fallback(\n\u001b[0m\u001b[0;32m    502\u001b[0m           \u001b[0mcheckpoint_prefixes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m           delete_old_dirs=delete_old_dirs, name=name, ctx=_ctx)\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints_eager_fallback\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name, ctx)\u001b[0m\n\u001b[0;32m    524\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_prefixes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"delete_old_dirs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelete_old_dirs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m   _result = _execute.execute(b\"MergeV2Checkpoints\", 0, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m    527\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m    528\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vikas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]"
     ]
    }
   ],
   "source": [
    "train(train_data,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ffeed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
